# AI-Powered UiPath PDD Generator: Complete Implementation Guide for LLM Coding Agent

## 1. Introduction & Overall Goal

This document outlines the complete implementation plan for an AI-powered application designed to generate UiPath-standard Process Design Documents (PDDs) from various business inputs (documents, videos, text guides). The goal is to automate the initial, labor-intensive phase of PDD creation, allowing RPA teams to focus on high-value analysis and development.

The application will accept user inputs, process them using specialized AI agents, populate a structured PDD template, generate visual diagrams, and facilitate an interactive review process.

## 2. High-Level Architecture

The system will be composed of four main layers:

1.  **Frontend (UI):** A React-based web application for user interaction, file uploads, chat, and document review.
2.  **Backend API:** A FastAPI (Python) service that handles HTTP requests, orchestrates the AI agents, and manages application state.
3.  **Agent Orchestrator:** A Python module within the backend that delegates tasks to specialized AI agents based on input type.
4.  **AI Model Layer:** External services (primarily OpenAI API) and local models for processing text, audio, and video.

## 3. Technology Stack

*   **Frontend:** React, JavaScript/TypeScript, Material-UI, Mermaid.js
*   **Backend:** Python, FastAPI, Uvicorn
*   **AI/LLM:** LangChain, OpenAI API (GPT-4o, GPT-4o, Whisper), OpenAI Python Library
*   **Document/Video Processing:** `python-docx`, `pypdf`, `ffmpeg-python`
*   **Templating:** Jinja2
*   **Database (Future Phases):** PostgreSQL, Vector DB (e.g., ChromaDB)
*   **Deployment:** Docker, GitHub Actions

---

## Phase 1: Foundation (MVP - Text Input Only)

### Objective

To build a Minimum Viable Product (MVP) that can accept a text-based description of a process and generate a basic, structured PDD.

### Core Tasks

1.  **Setup Project Structure and Basic Web Application:** Initialize the frontend and backend projects.
2.  **Create a Structured PDD Template:** Define the PDD sections and a Jinja2 template for rendering.
3.  **Develop the Text Processing Agent:** Build the core logic to extract information from text using an LLM.
4.  **Implement the PDD Generation API Endpoint:** Create an endpoint that ties the text agent and template together.

### Technology & Tools

*   **Languages/Frameworks:** Python, FastAPI, React
*   **Libraries:** `langchain-openai`, `jinja2`, `axios` (for frontend)
*   **AI Model:** OpenAI `gpt-4o`

---

### Detailed Prompts for LLM Coding Agent (Phase 1)

#### **Task 1: Setup Project Structure and Basic Web Application**

**Prompt:**
"Create a full-stack project with the following directory structure:

```
/pdd-generator
|-- /backend
|   |-- main.py
|   |-- requirements.txt
|   |-- /app
|       |-- __init__.py
|       |-- api
|       |   |-- __init__.py
|       |   |-- endpoints.py
|       |-- agents
|       |   |-- __init__.py
|       |   |-- text_agent.py
|       |-- templates
|       |   |-- pdd_template.html
|       |-- core
|       |   |-- __init__.py
|       |   |-- pdd_structure.yaml
|
|-- /frontend
|   |-- /src
|   |   |-- App.js
|   |   |-- index.js
|   |-- package.json
|
|-- README.md
```

1.  **Backend:** Initialize a Python virtual environment. In `backend/requirements.txt`, add `fastapi`, `uvicorn[standard]`, `python-dotenv`, `jinja2`, and `langchain-openai`. In `backend/main.py`, create a basic FastAPI application that imports `endpoints.py`.
2.  **Frontend:** In the `frontend` directory, initialize a new React application using `npx create-react-app .`. Install `axios` for making API calls.
3.  **CORS:** Configure the FastAPI backend to allow CORS (Cross-Origin Resource Sharing) for `http://localhost:3000`."

#### **Task 2: Create a Structured PDD Template**

**Prompt:**
"1. In `backend/app/core/pdd_structure.yaml`, define the structure of the PDD using YAML. Each section should have a `name` and a `prompt` for the LLM. Use the following structure:

```yaml
sections:
  - name: "Process Name"
    prompt: "Extract the official name of the process from the provided text. If no official name is found, create a concise and descriptive one."
  - name: "Process Description"
    prompt: "Summarize the overall purpose and goal of this process in 2-3 sentences."
  - name: "Actors & Systems"
    prompt: "List all the human roles (actors) and software systems involved in this process. Format as a bulleted list."
  - name: "Input Data"
    prompt: "Identify and list all the primary data inputs required for this process (e.g., invoice PDF, email, spreadsheet)."
  - name: "Output Data"
    prompt: "Identify and list all the primary outputs generated by this process (e.g., updated record, confirmation email, report)."
  - name: "Detailed Process Steps"
    prompt: "Break down the process into a detailed, numbered list of sequential steps. Be very specific about actions, like 'Click the 'Submit' button' or 'Enter the invoice number into the 'InvoiceID' field'."
  - name: "Business Rules & Exceptions"
    prompt: "List all explicit business rules, conditions, or constraints mentioned in the text. Also, identify any potential exceptions or error conditions (e.g., 'If invoice amount > $1000, require manager approval'). Format as a bulleted list."
```

2. In `backend/app/templates/pdd_template.html`, create a basic HTML document using the Jinja2 templating engine. It should loop through the generated data and display each section. Example:

```html
<!DOCTYPE html>
<html lang="en">
<head>
    <meta charset="UTF-8">
    <title>{{ process_name }}</title>
    <style>
        body { font-family: sans-serif; line-height: 1.6; padding: 20px; }
        h1, h2 { color: #2c3e50; }
        ul { list-style-type: square; }
        .section { margin-bottom: 30px; border-bottom: 1px solid #eee; padding-bottom: 20px; }
    </style>
</head>
<body>
    <h1>Process Design Document (PDD)</h1>

    {% for section in sections %}
    <div class="section">
        <h2>{{ section.name }}</h2>
        <div>{{ section.content | safe }}</div>
    </div>
    {% endfor %}

</body>
</html>
```
"

#### **Task 3: Develop the Text Processing Agent**

**Prompt:**
"In `backend/app/agents/text_agent.py`, create a Python script to process text and extract PDD sections.

1.  Create a function `load_pdd_structure()` that reads and parses `pdd_structure.yaml`.
2.  Create a main function `extract_pdd_sections(text_content: str) -> dict:` that does the following:
    a. Instantiates the OpenAI chat model: `from langchain_openai import ChatOpenAI; llm = ChatOpenAI(model="gpt-4o", temperature=0)`.
    b. Loads the PDD structure using `load_pdd_structure()`.
    c. Iterates through each section in the structure.
    d. For each section, construct a detailed prompt for the LLM. The prompt should include the section-specific prompt from the YAML file and the full `text_content`.
    e. Invoke the LLM and capture the response.
    f. Store the results in a dictionary where keys are section names and values are the LLM-generated content.
3.  Ensure the function returns the dictionary in a format that can be easily used by the Jinja2 template, like `[{'name': 'Process Name', 'content': '...'}, {'name': '...', 'content': '...'}]`.
4.  Add error handling (e.g., try-except block for API calls)."

#### **Task 4: Implement the PDD Generation API Endpoint**

**Prompt:**
"In `backend/app/api/endpoints.py`, create a new API endpoint `/generate-pdd` that accepts POST requests.

1.  The endpoint should expect a JSON body with a single key: `{'process_text': '...'}`.
2.  Inside the endpoint function:
    a. Import the `extract_pdd_sections` function from `app.agents.text_agent`.
    b. Import `Jinja2Templates` and configure it to look in the `app/templates` directory.
    c. Call `extract_pdd_sections` with the `process_text` from the request.
    d. Use the Jinja2 `Template` class to load `pdd_template.html`.
    e. Render the template with the extracted data.
    f. Return the rendered HTML as a `HTMLResponse`.
3.  Update the `main.py` to include this new router.
4.  On the React frontend, create a simple form with a textarea for the process text and a 'Generate PDD' button. When clicked, it should make a POST request to `http://localhost:8000/generate-pdd` and display the returned HTML in an iframe or a dedicated div using `dangerouslySetInnerHTML`."

---

## Phase 2: Multi-Modal Input (Documents & Video)

### Objective

To extend the MVP to handle more complex, real-world inputs: PDF documents, Word documents, and screen recordings (video).

### Core Tasks

1.  **Extend File Upload and Parsing:** Modify the backend to accept `.pdf`, `.docx`, and `.mp4` files and parse them into text.
2.  **Develop the Video Processing Agent:** Create a sophisticated agent to transcribe audio and analyze visual actions from screen recordings.
3.  **Integrate Multi-Modal Inputs:** Update the main generation endpoint to route different file types to the correct processing agent.

### Technology & Tools

*   **New Libraries:** `pypdf`, `python-docx`, `ffmpeg-python`, `openai` (for Whisper)
*   **AI Models:** OpenAI `whisper-1`, OpenAI `gpt-4o` (for vision)

---

### Detailed Prompts for LLM Coding Agent (Phase 2)

#### **Task 1: Extend File Upload and Parsing**

**Prompt:**
"1. Update the frontend to allow file uploads for `.pdf`, `.docx`, and `.mp4` types. Use `FormData` to send the file to the backend.
2. In the backend, create a new utility file `backend/app/utils/file_parser.py`.
3. In `file_parser.py`, create a function `parse_document(file_path: str) -> str`:
    a. If the file extension is `.docx`, use the `python-docx` library to extract all paragraphs and return them as a single string.
    b. If the file extension is `.pdf`, use the `pypdf` library to extract text from all pages.
    c. Add a placeholder for handling scanned PDFs, mentioning that an OCR library like `pytesseract` would be needed.
4. Create a new API endpoint `/upload-and-process` that accepts `multipart/form-data`. It should save the uploaded file temporarily, call `parse_document`, and then feed the resulting text into the existing `extract_pdd_sections` function from Phase 1."

#### **Task 2: Develop the Video Processing Agent**

**Prompt:**
"This is a complex task. Create a new file `backend/app/agents/video_agent.py`.

1.  **Function 1: Transcribe Audio**
    a. Create `transcribe_audio_from_video(video_path: str) -> str`.
    b. This function should use `ffmpeg-python` to extract the audio from the video file into a temporary `.wav` or `.mp3` file.
    c. Use the OpenAI Whisper API (`client.audio.transcriptions.create`) to transcribe the audio file. Request the output with timestamps (`timestamp_granularities=['word']`).
    d. Return the full transcript text.

2.  **Function 2: Analyze Video Frames**
    a. Create `analyze_video_frames(video_path: str, transcript: str) -> str`.
    b. Use `ffmpeg-python` to extract key frames from the video. A good strategy is to extract 1 frame every 3-5 seconds.
    c. Instantiate a multimodal model: `from langchain_openai import ChatOpenAI; vision_model = ChatOpenAI(model="gpt-4o", temperature=0)`.
    d. Loop through the extracted frames. For each frame:
        i. Read the image data in base64 format.
        ii. Construct a prompt for the vision model:
            ```python
            vision_prompt = f"""
            You are an expert RPA analyst analyzing a screen recording.
            The transcript of the video is: '{transcript}'.
            Analyze this screenshot and describe the specific UI action being performed at this moment.
            Focus on: What is being clicked? What data is being typed? What is being selected?
            Be concise and descriptive.
            """
            ```
        iii. Call the vision model with the image and the prompt. Store the result.
    e. Combine all the frame analyses into a single, chronological list of visual actions.

3.  **Function 3: Synthesize Information**
    a. Create `synthesize_video_analysis(transcript: str, visual_actions: str) -> str`.
    b. This function will use a standard LLM (`gpt-4o`) to combine the transcript and the visual actions into a single, coherent, step-by-step guide.
    c. Use a prompt like:
        ```python
        synthesis_prompt = f"""
        You are an expert UiPath Business Analyst. Your task is to create a detailed, step-by-step text guide from a screen recording analysis.
        You have two sources of information:
        1. The audio transcript: {transcript}
        2. The visual analysis of actions: {visual_actions}

        Combine these two sources to produce a single, clear, and accurate numbered list of steps for the process.
        Resolve any discrepancies between the audio and visual information.
        The output should be a detailed guide that can be used to build a PDD.
        """
        ```
    d. Return the synthesized guide."

#### **Task 3: Integrate Multi-Modal Inputs**

**Prompt:**
"Update the `/upload-and-process` endpoint in `endpoints.py` to handle video files.

1.  Check the file extension of the uploaded file.
2.  If it's a video (`.mp4`, `.mov`, etc.):
    a. Call the three functions from `video_agent.py` in sequence: `transcribe_audio_from_video`, `analyze_video_frames`, and `synthesize_video_analysis`.
    b. The final output from `synthesize_video_analysis` is your `process_text`.
3.  If it's a document (`.pdf`, `.docx`), use the `parse_document` function.
4.  Feed the resulting `process_text` (from either source) into the `extract_pdd_sections` function from `text_agent.py`.
5.  Ensure the frontend provides feedback to the user during this potentially long-running process (e.g., 'Processing video...')."

---

## Phase 3: Advanced Features & Refinement

### Objective

To add significant value by incorporating automated diagram generation and an interactive, human-in-the-loop review process.

### Core Tasks

1.  **Generate UML/Flowchart Diagrams:** Create an agent to convert process steps into a Mermaid.js diagram.
2.  **Implement Interactive Review Interface:** Allow users to edit PDD sections and request AI-driven refinements.
3.  **Add a Clarification Chat:** Enable users to provide additional context or ask questions during the generation process.

### Technology & Tools

*   **New Libraries:** `mermaid` (for rendering on frontend)
*   **Frontend:** Advanced state management might be needed (e.g., React Context or Redux).
*   **Backend:** New API endpoints for refinement.

---

### Detailed Prompts for LLM Coding Agent (Phase 3)

#### **Task 1: Generate UML/Flowchart Diagrams**

**Prompt:**
"1. In `backend/app/agents/`, create a new file `diagram_agent.py`.
2. Create a function `generate_mermaid_diagram(process_steps: str) -> str`:
    a. The input `process_steps` will be the content from the 'Detailed Process Steps' section of the PDD.
    b. Use an LLM with a carefully crafted prompt to convert these steps into Mermaid.js syntax.
    c. The prompt should be:
        ```python
        diagram_prompt = f"""
        You are an expert in business process modeling.
        Convert the following process steps into a Mermaid.js flowchart diagram.
        Use the `graph TD` (top-down) syntax.
        - Represent each step as a node (e.g., `A[Step 1]`).
        - Represent decision points as diamond shapes (e.g., `B{Is it valid?}`).
        - Use arrows (`-->`) to connect the nodes in sequence.
        - Keep the node text concise.
        - ONLY output the valid Mermaid.js code block, nothing else. Do not include markdown formatting like ```mermaid.

        Process Steps:
        {process_steps}
        """
        ```
    d. Return the raw Mermaid code string.
3. Integrate this into the main generation flow. After extracting PDD sections, call this new function with the 'Detailed Process Steps' content and add the result to the data passed to the Jinja template.
4. On the frontend, in the component that displays the PDD, add a check. If a `diagram_code` is present in the data, render it using the `mermaid.js` library. You will need to install `mermaid` (`npm install mermaid`) and initialize it."

#### **Task 2: Implement Interactive Review Interface**

**Prompt:**
"1. **Backend:** Create a new API endpoint `/refine-section`.
    a. This endpoint should accept a POST request with a JSON body: `{'section_name': '...', 'current_content': '...', 'user_feedback': '...'}`.
    b. Create a function `refine_content(section_name, current_content, user_feedback)` that uses an LLM to rewrite the content based on feedback.
    c. The prompt should be:
        ```python
        refine_prompt = f"""
        You are an expert UiPath Business Analyst refining a PDD section.
        The section name is: '{section_name}'.
        The current content is: '{current_content}'.
        The user has provided the following feedback: '{user_feedback}'.

        Rewrite the section content based on the user's feedback. Maintain a professional tone and adhere to UiPath documentation standards.
        Output only the revised content, nothing else.
        """
        ```
    d. Return the new, refined content from the endpoint.

2.  **Frontend:** Modify the PDD display component.
    a. Instead of just displaying text, render each section inside an editable container (e.g., a `<textarea>` or a content-editable `div`).
    b. Below each section, add an 'AI Refine' button and a text input for user feedback.
    c. When 'AI Refine' is clicked, make a POST request to the `/refine-section` endpoint with the section name, current content, and user feedback.
    d. On success, replace the section's content with the refined version returned from the API."

#### **Task 3: Add a Clarification Chat**

**Prompt:**
"1. **Backend:** Create a new endpoint `/chat`.
    a. This endpoint will manage a conversational state. For an MVP, the state can be a simple list of messages in memory. For a more robust solution, use a database.
    b. The endpoint should accept a POST with `{'message': '...'}`.
    c. It should add the user's message to the history and then get an LLM response.
    d. The LLM prompt needs to be aware of the current PDD context. It should be something like:
        ```python
        chat_prompt = f"""
        You are a helpful assistant for an expert UiPath Business Analyst creating a Process Design Document (PDD).
        The current PDD information is as follows:
        {json.dumps(current_pdd_data)}

        The user asks: '{user_message}'

        Based on the current PDD context, answer the user's question. If they ask to add information, identify the correct PDD section and suggest the new text.
        """
        ```
    e. Return the AI's response.

2.  **Frontend:** Add a chat-like interface to the PDD generation page.
    a. Display the history of messages (user and AI).
    b. Have an input box and a send button.
    c. When a message is sent, post it to `/chat` and update the message history with the response."

---

## Phase 4: Integration and Deployment

### Objective

To prepare the application for production use by integrating with external systems and ensuring it is scalable, secure, and maintainable.

### Core Tasks

1.  **Implement User Authentication & Security:** Add a basic login system.
2.  **Containerize with Docker:** Package the application for easy deployment.
3.  **Set up CI/CD Pipeline:** Automate testing and deployment.
4.  **Add UiPath Export Feature:** Create an endpoint to export the PDD in a structured format.

### Technology & Tools

*   **Security:** `python-jose` (JWT), `passlib` (password hashing)
*   **Deployment:** Docker, Docker Compose, GitHub Actions
*   **Export:** `openpyxl` (for Excel), or structured JSON

---

### Detailed Prompts for LLM Coding Agent (Phase 4)

#### **Task 1: Implement User Authentication & Security**

**Prompt:**
"1. **Backend:** Implement JWT (JSON Web Token) based authentication.
    a. Install `python-jose[cryptography]`, `passlib[bcrypt]`, and `python-multipart`.
    b. Create a simple user model (can be a dictionary for MVP, or use a database like SQLAlchemy with PostgreSQL).
    c. Create endpoints `/register` and `/login`. `/login` should verify credentials and return a JWT.
    d. Create a dependency `get_current_user` that can be used to protect other endpoints. This dependency will decode the JWT from the `Authorization` header.
    e. Apply this dependency to the `/generate-pdd`, `/upload-and-process`, and `/refine-section` endpoints.
2. **Frontend:** Implement a login/register page.
    a. On successful login, store the received JWT in `localStorage` or a secure cookie.
    b. Create an Axios interceptor that automatically attaches the JWT to the `Authorization` header for all subsequent API requests.
    c. Redirect unauthenticated users to the login page."

#### **Task 2: Containerize with Docker**

**Prompt:**
"1. Create a `Dockerfile` in the `backend` directory:
    a. Start from a Python base image (e.g., `python:3.10-slim`).
    b. Set the working directory.
    c. Copy `requirements.txt` and install dependencies.
    d. Copy the rest of the application code.
    e. Expose port 8000.
    f. Define the command to run the Uvicorn server.
2. Create a `Dockerfile` in the `frontend` directory:
    a. Use a multi-stage build. First stage: build the React app using a Node image. Second stage: copy the build artifacts into an Nginx image to serve them.
    b. Expose port 80 or 3000.
3. Create a `docker-compose.yml` file in the root directory to orchestrate the backend and frontend containers. Ensure they can communicate with each other."

#### **Task 3: Set up CI/CD Pipeline**

**Prompt:**
"Create a GitHub Actions workflow file at `.github/workflows/main.yml`.

1.  **Trigger:** Configure it to run on push to the `main` branch.
2.  **Jobs:**
    a. **test-and-build-backend:**
        i. Check out the code.
        ii. Set up Python.
        iii. Install dependencies.
        iv. (Optional but recommended) Run tests: `pytest`.
        v. Build the Docker image for the backend and push it to a container registry (e.g., GitHub Container Registry).
    b. **test-and-build-frontend:**
        i. Check out the code.
        ii. Set up Node.js.
        iii. Install dependencies.
        iv. (Optional) Run tests: `npm test`.
        v. Build the React app.
        vi. Build the Docker image for the frontend and push it.
    c. **deploy:**
        i. Depends on the build jobs.
        ii. Connect to your production server (e.g., via SSH).
        iii. Pull the new Docker images.
        iv. Run `docker-compose up -d` to restart the services with the new images."

#### **Task 4: Add UiPath Export Feature**

**Prompt:**
"1. **Backend:** Create a new endpoint `/export-pdd/{format}`.
    a. The `format` can be `json` or `xlsx`.
    b. This endpoint should be protected by authentication.
    c. It will need to retrieve the final PDD data. This might require passing a PDD ID in the request or storing it temporarily in a user session.
    d. If `format` is `json`, return the PDD data as a JSON file with the correct headers to trigger a download.
    e. If `format` is `xlsx`:
        i. Install the `openpyxl` library.
        ii. Create a function that takes the PDD data and writes it to an Excel file, with each section in a different row or sheet.
        iii. Return the Excel file with the correct headers.
2. **Frontend:** Add 'Export as JSON' and 'Export as Excel' buttons to the PDD review page. When clicked, they should call the respective export endpoint, trigger the file download in the browser."

